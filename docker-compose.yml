version: '3.8'

services:
  # Pili Chatbot API
  pili-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: pili-chatbot
    ports:
      - "8991:8991"
    env_file:
      - ./.env
    environment:
      # Health monitoring
      - HEALTH_CHECK_INTERVAL=30
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
      - ./.env:/app/.env:ro
    # depends_on:
    #   - redis
    #   - vllm-server
    restart: unless-stopped
    networks:
      - pili-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8991/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # # vLLM Server (Local LLM option)
  # vllm-server:
  #   image: vllm/vllm-openai:latest
  #   container_name: pili-vllm
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     - VLLM_MODEL=microsoft/DialoGPT-medium
  #     - VLLM_HOST=0.0.0.0
  #     - VLLM_PORT=8000
  #   command: [
  #     "--model", "microsoft/DialoGPT-medium",
  #     "--host", "0.0.0.0", 
  #     "--port", "8000",
  #     "--served-model-name", "microsoft/DialoGPT-medium"
  #   ]
  #   volumes:
  #     - vllm-cache:/root/.cache/huggingface
  #   restart: unless-stopped
  #   networks:
  #     - pili-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 60s
  #   # Uncomment below if you have GPU support
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]

  # # Ollama Server (Alternative Local LLM option)
  # ollama-server:
  #   image: ollama/ollama:latest
  #   container_name: pili-ollama
  #   ports:
  #     - "11434:11434"
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0:11434
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   restart: unless-stopped
  #   networks:
  #     - pili-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 30s
  #   # Uncomment to automatically pull a model on startup
  #   # command: >
  #   #   sh -c "ollama serve & 
  #   #          sleep 10 && 
  #   #          ollama pull llama2:7b-chat &&
  #   #          wait"

  # # Redis for caching and session management
  # redis:
  #   image: redis:7-alpine
  #   container_name: pili-redis
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis-data:/data
  #   restart: unless-stopped
  #   networks:
  #     - pili-network
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3

  # # Nginx reverse proxy (optional)
  # nginx:
  #   image: nginx:alpine
  #   container_name: pili-nginx
  #   ports:
  #     - "80:80"
  #     - "443:443"
  #   volumes:
  #     - ./nginx.conf:/etc/nginx/nginx.conf:ro
  #     - ./ssl:/etc/nginx/ssl:ro
  #   depends_on:
  #     - pili-api
  #   restart: unless-stopped
  #   networks:
  #     - pili-network
  #   profiles:
  #     - nginx  # Only start with --profile nginx

  # # Monitoring with Prometheus (optional)
  # prometheus:
  #   image: prom/prometheus:latest
  #   container_name: pili-prometheus
  #   ports:
  #     - "9090:9090"
  #   volumes:
  #     - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
  #     - prometheus-data:/prometheus
  #   command:
  #     - '--config.file=/etc/prometheus/prometheus.yml'
  #     - '--storage.tsdb.path=/prometheus'
  #     - '--web.console.libraries=/usr/share/prometheus/console_libraries'
  #     - '--web.console.templates=/usr/share/prometheus/consoles'
  #     - '--web.enable-lifecycle'
  #   restart: unless-stopped
  #   networks:
  #     - pili-network
  #   profiles:
  #     - monitoring

  # # Grafana for monitoring dashboard (optional)
  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: pili-grafana
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=admin
  #   volumes:
  #     - grafana-data:/var/lib/grafana
  #     - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
  #     - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
  #   depends_on:
  #     - prometheus
  #   restart: unless-stopped
  #   networks:
  #     - pili-network
  #   profiles:
  #     - monitoring

volumes:
  vllm-cache:
    driver: local
  ollama-data:
    driver: local
  redis-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  pili-network:
    driver: bridge