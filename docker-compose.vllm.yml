version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: pili_vllm_container
    restart: unless-stopped
    
    # GPU support for NVIDIA
    runtime: nvidia
    
    # Port mapping
    ports:
      - "8995:8000"
    
    # IPC mode for better GPU performance
    ipc: host
    
    # Shared memory size (important for GPU workloads)
    shm_size: 10gb
    
    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    
    # Volume mounts for persistence
    volumes:
      - huggingface_cache:/root/.cache/huggingface
      - vllm_logs:/app/logs
    
    # Model configuration
    command: >
      --model ${VLLM_MODEL:-meta-llama/Llama-3.2-3B-Instruct}
      --host 0.0.0.0
      --port 8000
      --served-model-name ${VLLM_SERVED_MODEL_NAME:-llama-3.2-3b}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-4096}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTIL:-0.9}
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL:-1}
      --enable-prefix-caching
      --enable-auto-tool-choice
      --tool-call-parser llama3_json
      --dtype auto
      --trust-remote-code
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Resource limits (adjust based on your GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# Named volumes for persistence
volumes:
  huggingface_cache:
    driver: local
  vllm_logs:
    driver: local

# Network for potential integration with other services
networks:
  default:
    name: pili_network 